{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f98651c-41ef-4099-8f2c-2507e05dc83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 904 entries, 0 to 903\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Date    904 non-null    object\n",
      " 1   Url     904 non-null    object\n",
      " 2   Text    904 non-null    object\n",
      " 3   Mark    904 non-null    int64 \n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 28.4+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"aapl_news_content.csv\"  # Update with the correct path to your dataset\n",
    "data = pd.read_csv(file_path, encoding='utf-8-sig')\n",
    "\n",
    "# Examine the dataset structure\n",
    "print(\"Dataset Info:\")\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2789b9b8-6011-4145-bf01-c8427cb527f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 5 Rows of the Dataset:\n",
      "                               Date  \\\n",
      "0   January 25, 2024 — 03:33 am EST   \n",
      "1  February 03, 2024 — 01:07 pm EST   \n",
      "2  February 03, 2024 — 10:20 am EST   \n",
      "3  February 03, 2024 — 07:40 am EST   \n",
      "4  February 03, 2024 — 05:00 am EST   \n",
      "\n",
      "                                                 Url  \\\n",
      "0  https://www.nasdaq.com/articles/zacks-earnings...   \n",
      "1  https://www.nasdaq.com/articles/could-apple-ac...   \n",
      "2  https://www.nasdaq.com/articles/microsoft-tesl...   \n",
      "3  https://www.nasdaq.com/articles/forget-amd-in-...   \n",
      "4  https://www.nasdaq.com/articles/1-unstoppable-...   \n",
      "\n",
      "                                                Text  Mark  \n",
      "0  For Immediate Release\\nChicago, IL – January 2...     1  \n",
      "1  Thanks to its lineup of incredibly popular har...     1  \n",
      "2  In this podcast, Motley Fool host Dylan Lewis ...     1  \n",
      "3  Shares in Advanced Micro Devices (NASDAQ: AMD)...     1  \n",
      "4  Investing in the stock market is one of the mo...     1  \n"
     ]
    }
   ],
   "source": [
    "# Preview the first few rows\n",
    "print(\"\\nFirst 5 Rows of the Dataset:\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a589841-836b-4363-a984-884829bb9c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rows Where 'Text' Contains Only Integers:\n",
      "             Date                                                Url Text  \\\n",
      "14              2  https://www.nasdaq.com/articles/us-stocks-wall...    2   \n",
      "53              2  https://www.nasdaq.com/articles/us-stocks-wall...    2   \n",
      "77              2  https://www.nasdaq.com/articles/us-stocks-futu...    2   \n",
      "129             2  https://www.nasdaq.com/articles/us-stocks-wall...    2   \n",
      "130             0  https://www.nasdaq.com/articles/apple-q1-earni...    0   \n",
      "..            ...                                                ...  ...   \n",
      "899  DEC 19, 2023  https://www.nasdaq.com/articles/49.1-of-warren...    0   \n",
      "900  DEC 19, 2023  https://www.nasdaq.com/articles/3-tech-stocks-...    0   \n",
      "901  DEC 19, 2023  https://www.nasdaq.com/articles/2-top-warren-b...    0   \n",
      "902  DEC 19, 2023  https://www.nasdaq.com/articles/tsmc-to-promot...    0   \n",
      "903  DEC 18, 2023  https://www.nasdaq.com/articles/3-metaverse-st...    0   \n",
      "\n",
      "     Mark  \n",
      "14      2  \n",
      "53      2  \n",
      "77      2  \n",
      "129     2  \n",
      "130     0  \n",
      "..    ...  \n",
      "899     0  \n",
      "900     0  \n",
      "901     0  \n",
      "902     0  \n",
      "903     0  \n",
      "\n",
      "[761 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nRows Where 'Text' Contains Only Integers:\")\n",
    "integer_text_rows = data[data['Text'].astype(str).str.match(r'^\\d+$')]\n",
    "print(integer_text_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1c97af4-64ca-4037-9d5b-3302158dacec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary of Rows Where 'Text' Contains Only Integers:\n",
      "Text\n",
      "0    756\n",
      "2      5\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Statistics about rows with integers in the 'Text' column\n",
    "print(\"\\nSummary of Rows Where 'Text' Contains Only Integers:\")\n",
    "print(integer_text_rows['Text'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3308f485-b041-47d8-b1c5-3860d3d69dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Count of Rows with Non-Numeric Text vs. Numeric Text:\n",
      "Numeric Text Rows: 761\n",
      "Non-Numeric Text Rows: 143\n"
     ]
    }
   ],
   "source": [
    "# Check how many rows have meaningful text vs. unexpected integers\n",
    "print(\"\\nCount of Rows with Non-Numeric Text vs. Numeric Text:\")\n",
    "is_numeric = data['Text'].astype(str).str.match(r'^\\d+$')\n",
    "print(f\"Numeric Text Rows: {is_numeric.sum()}\")\n",
    "print(f\"Non-Numeric Text Rows: {len(data) - is_numeric.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b3bd7b0c-b6a1-4248-b565-76203956f1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load datasets\n",
    "news_data = pd.read_csv(\"aa_news_content.csv\", encoding=\"utf-8-sig\")\n",
    "stock_data = pd.read_csv(\"aa_stock_processed.csv\", parse_dates=[\"Date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1d33efce-5fe6-4c40-91e7-44bda13c5d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Date                                                Url  \\\n",
      "0    2024-01-17  https://www.nasdaq.com/articles/sp-futures-sli...   \n",
      "1    2024-01-17  https://www.nasdaq.com/articles/how-the-pieces...   \n",
      "2    2024-01-17  https://www.nasdaq.com/articles/sp-futures-tic...   \n",
      "3    2024-01-17  https://www.nasdaq.com/articles/alcoa-q4-23-ea...   \n",
      "4    2024-01-16  https://www.nasdaq.com/articles/chipmakers-lea...   \n",
      "...         ...                                                ...   \n",
      "2242 2023-04-06  https://www.nasdaq.com/articles/alcoa-aa-stock...   \n",
      "2243 2023-04-06  https://www.nasdaq.com/articles/guru-fundament...   \n",
      "2244 2023-04-06  https://www.nasdaq.com/articles/may-26th-optio...   \n",
      "2245 2023-04-06  https://www.nasdaq.com/articles/pre-market-mos...   \n",
      "2246 2023-04-04  https://www.nasdaq.com/articles/unusual-put-op...   \n",
      "\n",
      "                                                   Text  Mark  \n",
      "0     March S&P 500 E-Mini futures (ESH24) are trend...     1  \n",
      "1     Looking at the underlying holdings of the ETFs...     1  \n",
      "2     March S&P 500 E-Mini futures (ESH24) are trend...     1  \n",
      "3     (RTTNews) - Alcoa Corp. (AA) will host a confe...     1  \n",
      "4     Market indices were lower again today (followi...     1  \n",
      "...                                                 ...   ...  \n",
      "2242  Alcoa (AA) closed at $39.16 in the latest trad...     1  \n",
      "2243  Below is Validea's guru fundamental report for...     1  \n",
      "2244  Investors in Alcoa Corporation (Symbol: AA) sa...     1  \n",
      "2245  The NASDAQ 100 Pre-Market Indicator is down -5...     1  \n",
      "2246  On April 4, 2023 at 11:04:31 ET an unusually l...     1  \n",
      "\n",
      "[2247 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Function to clean and standardize the Date column\n",
    "def clean_date(date_str):\n",
    "    # Remove everything after the first '—' (time and timezone)\n",
    "    date_str = date_str.split('—')[0].strip()\n",
    "    # Convert to datetime format\n",
    "    try:\n",
    "        return pd.to_datetime(date_str, format=\"%B %d, %Y\", errors=\"coerce\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing date: {date_str}\")\n",
    "        return pd.NaT\n",
    "\n",
    "# Apply the cleaning function to the 'Date' column\n",
    "news_data['Date'] = news_data['Date'].apply(clean_date)\n",
    "\n",
    "# Preview the cleaned data\n",
    "print(news_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "96b30bbe-2da8-4e51-ae1e-be315a64a0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Date       Open       High        Low      Close  Adj close  \\\n",
      "0      2024-02-02  29.000000  29.719999  28.549999  29.490000  29.490000   \n",
      "1      2024-02-01  30.080000  30.405001  29.150000  29.690001  29.690001   \n",
      "2      2024-01-31  30.490000  31.360001  29.715000  29.750000  29.750000   \n",
      "3      2024-01-30  30.340000  30.840000  30.000000  30.610001  30.610001   \n",
      "4      2024-01-29  30.459999  30.969999  29.688999  30.910000  30.910000   \n",
      "...           ...        ...        ...        ...        ...        ...   \n",
      "13637  1970-01-08   6.971203   7.008750   6.921141   6.946172   1.896820   \n",
      "13638  1970-01-07   6.983719   6.983719   6.958688   6.971203   1.903655   \n",
      "13639  1970-01-06   7.083844   7.108875   7.008750   7.008750   1.913908   \n",
      "13640  1970-01-05   7.158938   7.209000   7.071328   7.083844   1.934414   \n",
      "13641  1970-01-02   7.158938   7.234031   7.158938   7.158938   1.954920   \n",
      "\n",
      "        Volume  \n",
      "0      4954000  \n",
      "1      4174600  \n",
      "2      5760400  \n",
      "3      4714700  \n",
      "4      4649100  \n",
      "...        ...  \n",
      "13637    57928  \n",
      "13638    73908  \n",
      "13639    92884  \n",
      "13640    25968  \n",
      "13641    22971  \n",
      "\n",
      "[13642 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# Convert to datetime and extract only the date (YYYY-MM-DD)\n",
    "stock_data['Date'] = pd.to_datetime(stock_data['Date'], errors='coerce').dt.date\n",
    "\n",
    "# Preview the cleaned data\n",
    "print(stock_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "758c75b4-666f-497d-ade5-4eac10ae2ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'Date' in news_data is properly converted to datetime\n",
    "news_data['Date'] = pd.to_datetime(news_data['Date'], errors='coerce')\n",
    "\n",
    "# Ensure 'Date' in stock_data is properly converted to datetime\n",
    "stock_data['Date'] = pd.to_datetime(stock_data['Date'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9b12ff5f-e28a-4c06-b362-39c4cbb3e96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New dataset saved as: merged_stock_news.csv\n",
      "        Date   Open   High        Low  Close  Adj close   Volume  \\\n",
      "0 2024-01-17  27.17  27.67  27.049999  27.18      27.18  9606400   \n",
      "1 2024-01-17  27.17  27.67  27.049999  27.18      27.18  9606400   \n",
      "2 2024-01-17  27.17  27.67  27.049999  27.18      27.18  9606400   \n",
      "3 2024-01-17  27.17  27.67  27.049999  27.18      27.18  9606400   \n",
      "4 2024-01-17  27.17  27.67  27.049999  27.18      27.18  9606400   \n",
      "\n",
      "                                                 Url  \\\n",
      "0  https://www.nasdaq.com/articles/sp-futures-sli...   \n",
      "1  https://www.nasdaq.com/articles/how-the-pieces...   \n",
      "2  https://www.nasdaq.com/articles/sp-futures-tic...   \n",
      "3  https://www.nasdaq.com/articles/alcoa-q4-23-ea...   \n",
      "4  https://www.nasdaq.com/articles/compared-to-es...   \n",
      "\n",
      "                                                Text  Mark Lagged_Date  \\\n",
      "0  March S&P 500 E-Mini futures (ESH24) are trend...     1  2024-01-18   \n",
      "1  Looking at the underlying holdings of the ETFs...     1  2024-01-18   \n",
      "2  March S&P 500 E-Mini futures (ESH24) are trend...     1  2024-01-18   \n",
      "3  (RTTNews) - Alcoa Corp. (AA) will host a confe...     1  2024-01-18   \n",
      "4  For the quarter ended December 2023, Alcoa (AA...     1  2024-01-18   \n",
      "\n",
      "  Date_lagged  \n",
      "0         NaT  \n",
      "1         NaT  \n",
      "2         NaT  \n",
      "3         NaT  \n",
      "4         NaT  \n"
     ]
    }
   ],
   "source": [
    "# Merge stock_data with same-day news\n",
    "merged_same_day = pd.merge(stock_data, news_data, on=\"Date\", how=\"inner\", suffixes=(\"\", \"_same\"))\n",
    "\n",
    "# Add a lagged date column to news_data\n",
    "news_data['Lagged_Date'] = news_data['Date'] + pd.Timedelta(days=1)\n",
    "\n",
    "# Merge stock_data with lagged news\n",
    "merged_lagged = pd.merge(stock_data, news_data, left_on=\"Date\", right_on=\"Lagged_Date\", how=\"inner\", suffixes=(\"\", \"_lagged\"))\n",
    "\n",
    "# Combine same-day and lagged-day data\n",
    "merged_combined = pd.concat([merged_same_day, merged_lagged], axis=0).drop_duplicates()\n",
    "\n",
    "# Save the combined dataset to a new CSV\n",
    "output_file_path = \"merged_stock_news.csv\"\n",
    "merged_combined.to_csv(output_file_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"New dataset saved as: {output_file_path}\")\n",
    "print(merged_combined.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d362eeef-e5a3-498a-80db-029561b2534e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted dataset saved as: sorted_merged_stock_news.csv\n"
     ]
    }
   ],
   "source": [
    "# Load the merged dataset\n",
    "merged_data = pd.read_csv(\"merged_stock_news.csv\", parse_dates=[\"Date\"])\n",
    "\n",
    "# Sort the dataset by date from old to new\n",
    "merged_data = merged_data.sort_values(by=\"Date\").reset_index(drop=True)\n",
    "\n",
    "# Save the sorted dataset\n",
    "sorted_file_path = \"sorted_merged_stock_news.csv\"\n",
    "merged_data.to_csv(sorted_file_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"Sorted dataset saved as: {sorted_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ed208505-b0be-4a3f-8e2f-9120c7aa96dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset saved as: train_stock_news.csv\n",
      "Test dataset saved as: test_stock_news.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into train (70%) and test (30%)\n",
    "train_data, test_data = train_test_split(merged_data, test_size=0.3, random_state=42, shuffle=False)\n",
    "\n",
    "# Save the train and test datasets\n",
    "train_file_path = \"train_stock_news.csv\"\n",
    "test_file_path = \"test_stock_news.csv\"\n",
    "\n",
    "train_data.to_csv(train_file_path, index=False, encoding=\"utf-8-sig\")\n",
    "test_data.to_csv(test_file_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"Train dataset saved as: {train_file_path}\")\n",
    "print(f\"Test dataset saved as: {test_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "aeede46f-579c-4167-a1ad-7107151ec002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\19793\\anaconda3\\lib\\site-packages (4.45.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: torch in c:\\users\\19793\\anaconda3\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\19793\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\19793\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\19793\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\19793\\anaconda3\\lib\\site-packages (from transformers) (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\19793\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\19793\\anaconda3\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\19793\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\19793\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\19793\\anaconda3\\lib\\site-packages (from transformers) (2.32.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\19793\\anaconda3\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\19793\\anaconda3\\lib\\site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\19793\\anaconda3\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\19793\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\19793\\anaconda3\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\19793\\anaconda3\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\19793\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\19793\\anaconda3\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\19793\\anaconda3\\lib\\site-packages (from torch) (69.5.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\19793\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\19793\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\19793\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\19793\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\19793\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\19793\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\19793\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\19793\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\19793\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\19793\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\19793\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\19793\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\19793\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\19793\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "pip install transformers torch scikit-learn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "83730702-6076-49e1-9e0a-d99599740467",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Custom Dataset for BERT\n",
    "class StockNewsDataset(Dataset):\n",
    "    def __init__(self, texts, prices, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.prices = prices\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        price = self.prices[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"target\": torch.tensor(price, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "# Prepare datasets for BERT\n",
    "train_dataset = StockNewsDataset(\n",
    "    texts=train_data[\"Text\"].tolist(),\n",
    "    prices=train_data[\"Close\"].tolist(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=128\n",
    ")\n",
    "\n",
    "test_dataset = StockNewsDataset(\n",
    "    texts=test_data[\"Text\"].tolist(),\n",
    "    prices=test_data[\"Close\"].tolist(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=128\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a2c9c5a4-988d-4b5c-83c9-d91fb10caff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertStockPredictor(nn.Module):\n",
    "    def __init__(self, bert_model):\n",
    "        super(BertStockPredictor, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.fc = nn.Linear(768, 1)  # 768 hidden size for BERT base, 1 output for regression\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
    "        return self.fc(cls_output)\n",
    "\n",
    "# Load pre-trained BERT\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertStockPredictor(bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "81cc5735-0a10-4f14-988d-fb5310e70ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 593.7792147997743\n",
      "Epoch 2, Loss: 493.8737049932065\n",
      "Epoch 3, Loss: 462.97387515239836\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()  # Mean Squared Error for regression\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(3):  # Adjust the number of epochs as needed\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        targets = batch[\"target\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask).squeeze()\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c1243b15-e066-466f-bdb2-309161da6770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 43.0177\n",
      "Predictions saved to predictions.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\19793\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "# Collect predictions and targets\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        targets = batch[\"target\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask).squeeze()\n",
    "        all_preds.extend(outputs.cpu().numpy())\n",
    "        all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = mean_squared_error(all_targets, all_preds, squared=False)\n",
    "print(f\"Test RMSE: {rmse:.4f}\")\n",
    "\n",
    "# Add predictions to the test dataset\n",
    "test_data = test_data.copy()  # Avoid modifying the original test_data\n",
    "test_data[\"prediction\"] = all_preds\n",
    "\n",
    "# Save the test data with predictions to a new CSV\n",
    "output_file_path = \"predictions.csv\"\n",
    "test_data.to_csv(output_file_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"Predictions saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9378a848-b640-42d0-aab6-5f3aa56244e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load and preprocess dataset\n",
    "merged_data = pd.read_csv(\"sorted_merged_stock_news.csv\", parse_dates=[\"Date\"])\n",
    "\n",
    "# Create Movement labels: 1 (Up), 0 (Down/No Change)\n",
    "merged_data[\"Movement\"] = (merged_data[\"Close\"] > merged_data[\"Open\"]).astype(int)\n",
    "\n",
    "# Drop rows with missing values in Text or Movement\n",
    "merged_data = merged_data.dropna(subset=[\"Text\", \"Movement\"])\n",
    "\n",
    "# Split dataset into train and test (70%-30%)\n",
    "train_data, test_data = train_test_split(merged_data, test_size=0.3, random_state=42, shuffle=False)\n",
    "\n",
    "# Extract texts and labels\n",
    "train_texts = train_data[\"Text\"].tolist()\n",
    "train_labels = train_data[\"Movement\"].tolist()\n",
    "\n",
    "test_texts = test_data[\"Text\"].tolist()\n",
    "test_labels = test_data[\"Movement\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81863338-86c4-47a9-9b6e-e1e91b75b90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Initialize BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Define custom dataset class for BERT\n",
    "class StockNewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Prepare train and test datasets\n",
    "train_dataset = StockNewsDataset(\n",
    "    texts=train_texts,\n",
    "    labels=train_labels,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=128\n",
    ")\n",
    "\n",
    "test_dataset = StockNewsDataset(\n",
    "    texts=test_texts,\n",
    "    labels=test_labels,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=128\n",
    ")\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1020ada4-8d4d-4c84-b713-873073692c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        pooled_output = outputs.pooler_output  # CLS token output\n",
    "        output = self.dropout(pooled_output)\n",
    "        return self.out(output)\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BertClassifier(n_classes=2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "046ead81-c515-4880-9fc7-b6da71b58008",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\19793\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.7015\n",
      "Epoch 2/5, Loss: 0.6894\n",
      "Epoch 3/5, Loss: 0.6592\n",
      "Epoch 4/5, Loss: 0.6028\n",
      "Epoch 5/5, Loss: 0.5539\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "from transformers import get_scheduler\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "epochs = 5  # Number of epochs\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=len(train_loader) * epochs)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "132d514d-743b-41bd-a095-a2420f40512e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.5146\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "Down/No Change       0.54      0.56      0.55       584\n",
      "            Up       0.48      0.46      0.47       520\n",
      "\n",
      "      accuracy                           0.52      1104\n",
      "     macro avg       0.51      0.51      0.51      1104\n",
      "  weighted avg       0.51      0.52      0.51      1104\n",
      "\n",
      "Predictions saved to prediction_updown.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        \n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate F1 Score\n",
    "f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(all_labels, all_preds, target_names=[\"Down/No Change\", \"Up\"]))\n",
    "\n",
    "# Save predictions to CSV\n",
    "test_data = test_data.copy()\n",
    "test_data[\"predictions\"] = all_preds\n",
    "test_data[\"predictions\"] = test_data[\"predictions\"].map({1: \"Up\", 0: \"Down/No Change\"})\n",
    "test_data[\"Movement\"] = test_data[\"Movement\"].map({1: \"Up\", 0: \"Down/No Change\"})\n",
    "\n",
    "# Save to CSV\n",
    "test_data.to_csv(\"prediction_updown.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"Predictions saved to prediction_updown.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58adb14b-35a7-4c7a-b394-5be81869486e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
