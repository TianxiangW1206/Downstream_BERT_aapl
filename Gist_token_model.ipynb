{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9902a656-3285-456f-a9c9-a37de5f5bc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "train_data = pd.read_csv(\"train_stock_news.csv\")\n",
    "test_data = pd.read_csv(\"test_stock_news.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "784c02ce-45be-4654-aa4a-4e78c4bc3c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.5.1-cp312-cp312-win_amd64.whl.metadata (28 kB)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.20.1-cp312-cp312-win_amd64.whl.metadata (6.2 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.5.1-cp312-cp312-win_amd64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\19793\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\19793\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\19793\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\19793\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\19793\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\19793\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\19793\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\19793\\anaconda3\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\19793\\anaconda3\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\19793\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Downloading torch-2.5.1-cp312-cp312-win_amd64.whl (203.0 MB)\n",
      "   ---------------------------------------- 0.0/203.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 10.0/203.0 MB 62.2 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 25.4/203.0 MB 67.1 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 41.2/203.0 MB 70.8 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 57.4/203.0 MB 73.2 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 73.4/203.0 MB 74.3 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 87.8/203.0 MB 73.8 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 96.2/203.0 MB 69.0 MB/s eta 0:00:02\n",
      "   -------------------- ------------------ 105.9/203.0 MB 65.7 MB/s eta 0:00:02\n",
      "   ---------------------- ---------------- 114.8/203.0 MB 63.2 MB/s eta 0:00:02\n",
      "   ----------------------- --------------- 124.8/203.0 MB 61.8 MB/s eta 0:00:02\n",
      "   ------------------------- ------------- 132.4/203.0 MB 59.6 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 140.8/203.0 MB 57.7 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 148.6/203.0 MB 56.2 MB/s eta 0:00:01\n",
      "   ----------------------------- --------- 153.4/203.0 MB 53.8 MB/s eta 0:00:01\n",
      "   ------------------------------- ------- 162.3/203.0 MB 52.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------ 169.9/203.0 MB 51.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 180.9/203.0 MB 51.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 193.2/203.0 MB 52.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.0 MB 53.2 MB/s eta 0:00:01\n",
      "   --------------------------------------- 203.0/203.0 MB 49.4 MB/s eta 0:00:00\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 6.2/6.2 MB 47.4 MB/s eta 0:00:00\n",
      "Downloading torchvision-0.20.1-cp312-cp312-win_amd64.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.6/1.6 MB 41.9 MB/s eta 0:00:00\n",
      "Downloading torchaudio-2.5.1-cp312-cp312-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.4/2.4 MB 27.9 MB/s eta 0:00:00\n",
      "Installing collected packages: sympy, torch, torchvision, torchaudio\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.2\n",
      "    Uninstalling sympy-1.13.2:\n",
      "      Successfully uninstalled sympy-1.13.2\n",
      "Successfully installed sympy-1.13.1 torch-2.5.1 torchaudio-2.5.1 torchvision-0.20.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c549bb6f-1753-4c0f-aa31-461386acf3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\19793\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Downloading huggingface_hub-0.26.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\19793\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\19793\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\19793\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\19793\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\19793\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers)\n",
      "  Downloading tokenizers-0.20.3-cp312-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Using cached safetensors-0.4.5-cp312-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\19793\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\19793\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\19793\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\19793\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\19793\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\19793\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\19793\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\19793\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Downloading transformers-4.46.3-py3-none-any.whl (10.0 MB)\n",
      "   ---------------------------------------- 0.0/10.0 MB ? eta -:--:--\n",
      "   ---------------------------------------  10.0/10.0 MB 56.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.0/10.0 MB 52.0 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.26.3-py3-none-any.whl (447 kB)\n",
      "Using cached safetensors-0.4.5-cp312-none-win_amd64.whl (286 kB)\n",
      "Downloading tokenizers-0.20.3-cp312-none-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.4/2.4 MB 132.4 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.26.3 safetensors-0.4.5 tokenizers-0.20.3 transformers-4.46.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b706d4e8-854b-49a4-84e9-51365fc58035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.nn import Module\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import AdamW\n",
    "\n",
    "# Initialize tokenizer and base model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "base_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Function to extract entities manually\n",
    "def extract_gist(text, date):\n",
    "    # Define regex patterns for dates, company names, and events\n",
    "    date_pattern = r\"\\b\\d{4}-\\d{2}-\\d{2}\\b\"  # Matches YYYY-MM-DD\n",
    "    company_pattern = r\"\\b[A-Z][a-zA-Z]+(?:\\s[A-Z][a-zA-Z]+)*\\b\"  # Matches proper nouns (e.g., \"Apple Inc.\")\n",
    "    event_keywords = [\"earnings\", \"merger\", \"report\", \"acquisition\", \"profit\", \"loss\", \"growth\", \"decline\"]\n",
    "\n",
    "    # Extract entities\n",
    "    dates = re.findall(date_pattern, text)\n",
    "    companies = re.findall(company_pattern, text)\n",
    "    events = [word for word in event_keywords if word in text.lower()]\n",
    "\n",
    "    # Create gist token\n",
    "    gist = f\"[GIST] {' '.join(companies)} {' '.join(events)} [DATE] {date}\"\n",
    "    return gist\n",
    "\n",
    "# Create Gist Token Dataset\n",
    "class GistTokenDataset(Dataset):\n",
    "    def __init__(self, texts, dates, tokenizer, max_len=512):\n",
    "        self.texts = texts\n",
    "        self.dates = dates\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        date = self.dates[idx]\n",
    "        gist = extract_gist(text, date)\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            gist + \" \" + text,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fb7d30d-8594-4aa9-9874-73811d754bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Dataset\n",
    "texts = train_data[\"Text\"].tolist()\n",
    "dates = train_data[\"Date\"].tolist()\n",
    "train_dataset = GistTokenDataset(texts, dates, tokenizer)\n",
    "\n",
    "# Define DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Custom Attention Modification Module\n",
    "class ModifiedAttentionModel(Module):\n",
    "    def __init__(self, base_model):\n",
    "        super(ModifiedAttentionModel, self).__init__()\n",
    "        self.base_model = base_model\n",
    "        self.attention_layer = torch.nn.Linear(768, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Freeze embedding and encoder layers\n",
    "        with torch.no_grad():\n",
    "            outputs = self.base_model(input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Add custom attention focusing on gist tokens\n",
    "        hidden_states = outputs.last_hidden_state  # Shape: (batch_size, seq_len, hidden_dim)\n",
    "        attention_scores = self.attention_layer(hidden_states).squeeze(-1)  # Shape: (batch_size, seq_len)\n",
    "        attention_weights = torch.nn.functional.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "        # Weighted sum of token representations\n",
    "        context_vector = torch.matmul(attention_weights.unsqueeze(1), hidden_states).squeeze(1)\n",
    "        return context_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3387b4ce-cbb0-477b-b2a9-85d3e5cdd6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\19793\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize modified model\n",
    "model = ModifiedAttentionModel(base_model).to(device)\n",
    "\n",
    "# Optimizer and Training Loop\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "def train_attention_model(model, dataloader, optimizer, epochs=3):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch in tqdm(dataloader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "            # Forward pass through the modified model\n",
    "            context_vector = model(input_ids, attention_mask)\n",
    "            loss = torch.mean(context_vector)  # Dummy loss for demonstration\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1} Loss: {epoch_loss / len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3c10c58-09ca-4a9e-ba0f-ba28db62a8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 161/161 [41:59<00:00, 15.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: -0.015062478932071917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 161/161 [41:56<00:00, 15.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: -0.019871406280291007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 161/161 [41:56<00:00, 15.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: -0.02361619373008331\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_attention_model(model, train_loader, optimizer)\n",
    "\n",
    "# Save the model for downstream tasks\n",
    "torch.save(model.state_dict(), \"modified_attention_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "511b08b1-7a3c-4e23-8418-ac912d022eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\19793\\AppData\\Local\\Temp\\ipykernel_14668\\3180771540.py:36: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"modified_attention_model.pth\", map_location=device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ModifiedAttentionModel(\n",
       "  (base_model): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (attention_layer): Linear(in_features=768, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define tokenizer and load the saved model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Define the custom model\n",
    "class ModifiedAttentionModel(torch.nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super(ModifiedAttentionModel, self).__init__()\n",
    "        self.base_model = base_model\n",
    "        self.attention_layer = torch.nn.Linear(768, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        with torch.no_grad():  # Freeze embeddings and encoder layers\n",
    "            outputs = self.base_model(input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        attention_scores = self.attention_layer(hidden_states).squeeze(-1)\n",
    "        attention_weights = torch.nn.functional.softmax(attention_scores, dim=-1)\n",
    "        context_vector = torch.matmul(attention_weights.unsqueeze(1), hidden_states).squeeze(1)\n",
    "        return context_vector\n",
    "\n",
    "# Load the trained model\n",
    "base_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "model = ModifiedAttentionModel(base_model)\n",
    "model.load_state_dict(torch.load(\"modified_attention_model.pth\", map_location=device))\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cb7d88f-a598-463c-a8d2-b7d8ee6b5d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset for embedding extraction\n",
    "class GistEmbeddingDataset(Dataset):\n",
    "    def __init__(self, texts, dates, tokenizer, max_len=512):\n",
    "        self.texts = texts\n",
    "        self.dates = dates\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        date = self.dates[idx]\n",
    "        gist = f\"[GIST] [DATE] {date}\"\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            gist + \" \" + text,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc4655dd-7548-4588-90fd-4278eb0f81f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train and test data\n",
    "train_data = pd.read_csv(\"train_stock_news.csv\")\n",
    "test_data = pd.read_csv(\"test_stock_news.csv\")\n",
    "\n",
    "# Split validation set from the training set\n",
    "val_split = 0.2\n",
    "\n",
    "# Calculate labels using Open and Close prices\n",
    "train_labels = (train_data[\"Close\"] - train_data[\"Open\"] > 0).astype(int).tolist()\n",
    "test_labels = (test_data[\"Close\"] - test_data[\"Open\"] > 0).astype(int).tolist()\n",
    "\n",
    "train_texts = train_data[\"Text\"].tolist()\n",
    "train_dates = train_data[\"Date\"].tolist()\n",
    "\n",
    "train_texts, val_texts, train_dates, val_dates, train_labels, val_labels = train_test_split(\n",
    "    train_texts, train_dates, train_labels, test_size=val_split, random_state=42\n",
    ")\n",
    "\n",
    "test_texts = test_data[\"Text\"].tolist()\n",
    "test_dates = test_data[\"Date\"].tolist()\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = GistEmbeddingDataset(train_texts, train_dates, tokenizer)\n",
    "val_dataset = GistEmbeddingDataset(val_texts, val_dates, tokenizer)\n",
    "test_dataset = GistEmbeddingDataset(test_texts, test_dates, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c890365b-6b7f-4bec-a7ce-d9593815442b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Embeddings: 100%|██████████| 129/129 [26:45<00:00, 12.44s/it]\n",
      "Extracting Embeddings: 100%|██████████| 33/33 [06:41<00:00, 12.18s/it]\n",
      "Extracting Embeddings: 100%|██████████| 69/69 [14:20<00:00, 12.47s/it]\n"
     ]
    }
   ],
   "source": [
    "# Function to extract embeddings\n",
    "def extract_embeddings(model, dataloader):\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Extracting Embeddings\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            context_vector = model(input_ids, attention_mask)\n",
    "            embeddings.append(context_vector.cpu().numpy())\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "# Extract embeddings for train, validation, and test sets\n",
    "train_embeddings = extract_embeddings(model, train_loader)\n",
    "val_embeddings = extract_embeddings(model, val_loader)\n",
    "test_embeddings = extract_embeddings(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66dbfc64-299c-410a-8798-8ba3681b44e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5650485436893203\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.62      0.60       266\n",
      "           1       0.56      0.51      0.53       249\n",
      "\n",
      "    accuracy                           0.57       515\n",
      "   macro avg       0.56      0.56      0.56       515\n",
      "weighted avg       0.56      0.57      0.56       515\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train a Random Forest classifier on the extracted embeddings\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier.fit(train_embeddings, train_labels)\n",
    "\n",
    "# Make predictions on validation set\n",
    "val_predictions = rf_classifier.predict(val_embeddings)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "print(\"Validation Accuracy:\", val_accuracy)\n",
    "print(\"Validation Classification Report:\")\n",
    "print(classification_report(val_labels, val_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a06f23fe-0bf3-425f-a15b-80ebfd76a54b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5371376811594203\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.73      0.63       584\n",
      "           1       0.51      0.32      0.39       520\n",
      "\n",
      "    accuracy                           0.54      1104\n",
      "   macro avg       0.53      0.53      0.51      1104\n",
      "weighted avg       0.53      0.54      0.52      1104\n",
      "\n",
      "Predictions saved to stock_price_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on test set\n",
    "test_predictions = rf_classifier.predict(test_embeddings)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(test_labels, test_predictions))\n",
    "\n",
    "# Save the predictions for analysis\n",
    "output = pd.DataFrame({\"Text\": test_texts, \"True Label\": test_labels, \"Predicted Label\": test_predictions})\n",
    "output.to_csv(\"stock_price_predictions.csv\", index=False)\n",
    "print(\"Predictions saved to stock_price_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e62c47a9-1f86-40e0-b978-16822182295b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\19793\\AppData\\Local\\Temp\\ipykernel_14668\\2242396656.py:38: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"modified_attention_model.pth\", map_location=device))\n"
     ]
    }
   ],
   "source": [
    "# ---------improve the model, using close price only--------\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define tokenizer and load the saved model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Define the custom model\n",
    "class ModifiedAttentionModel(torch.nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super(ModifiedAttentionModel, self).__init__()\n",
    "        self.base_model = base_model\n",
    "        self.attention_layer = torch.nn.Linear(768, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        with torch.no_grad():  # Freeze embeddings and encoder layers\n",
    "            outputs = self.base_model(input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        attention_scores = self.attention_layer(hidden_states).squeeze(-1)\n",
    "        attention_weights = torch.nn.functional.softmax(attention_scores, dim=-1)\n",
    "        context_vector = torch.matmul(attention_weights.unsqueeze(1), hidden_states).squeeze(1)\n",
    "        return context_vector\n",
    "\n",
    "# Load the trained model\n",
    "base_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "model = ModifiedAttentionModel(base_model)\n",
    "model.load_state_dict(torch.load(\"modified_attention_model.pth\", map_location=device))\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# Define dataset for embedding extraction\n",
    "class GistEmbeddingDataset(Dataset):\n",
    "    def __init__(self, texts, dates, tokenizer, max_len=512):\n",
    "        self.texts = texts\n",
    "        self.dates = dates\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        date = self.dates[idx]\n",
    "        gist = f\"[GIST] [DATE] {date}\"\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            gist + \" \" + text,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf776301-fa3e-4dbc-9d32-131dc7ef6ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train and test data\n",
    "train_data = pd.read_csv(\"train_stock_news.csv\")\n",
    "test_data = pd.read_csv(\"test_stock_news.csv\")\n",
    "\n",
    "# Split validation set from the training set\n",
    "val_split = 0.2\n",
    "\n",
    "# Calculate labels\n",
    "train_labels = (train_data[\"Close\"].diff().fillna(0) > 0).astype(int).tolist()\n",
    "test_labels = (test_data[\"Close\"].diff().fillna(0) > 0).astype(int).tolist()\n",
    "\n",
    "train_texts = train_data[\"Text\"].tolist()\n",
    "train_dates = train_data[\"Date\"].tolist()\n",
    "\n",
    "train_texts, val_texts, train_dates, val_dates, train_labels, val_labels = train_test_split(\n",
    "    train_texts, train_dates, train_labels, test_size=val_split, random_state=42\n",
    ")\n",
    "\n",
    "test_texts = test_data[\"Text\"].tolist()\n",
    "test_dates = test_data[\"Date\"].tolist()\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = GistEmbeddingDataset(train_texts, train_dates, tokenizer)\n",
    "val_dataset = GistEmbeddingDataset(val_texts, val_dates, tokenizer)\n",
    "test_dataset = GistEmbeddingDataset(test_texts, test_dates, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7b1b6d9-5773-45b5-b5b0-2a638b8ca1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Embeddings: 100%|██████████| 129/129 [48:25<00:00, 22.52s/it]\n",
      "Extracting Embeddings: 100%|██████████| 33/33 [12:03<00:00, 21.92s/it]\n",
      "Extracting Embeddings: 100%|██████████| 69/69 [24:54<00:00, 21.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 81 candidates, totalling 243 fits\n",
      "Validation Accuracy: 0.7281553398058253\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.84      0.83       413\n",
      "           1       0.29      0.26      0.28       102\n",
      "\n",
      "    accuracy                           0.73       515\n",
      "   macro avg       0.56      0.55      0.56       515\n",
      "weighted avg       0.72      0.73      0.72       515\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to extract embeddings\n",
    "def extract_embeddings(model, dataloader):\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Extracting Embeddings\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            context_vector = model(input_ids, attention_mask)\n",
    "            embeddings.append(context_vector.cpu().numpy())\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "# Extract embeddings for train, validation, and test sets\n",
    "train_embeddings = extract_embeddings(model, train_loader)\n",
    "val_embeddings = extract_embeddings(model, val_loader)\n",
    "test_embeddings = extract_embeddings(model, test_loader)\n",
    "\n",
    "# Balance the training set using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "train_embeddings_balanced, train_labels_balanced = smote.fit_resample(train_embeddings, train_labels)\n",
    "\n",
    "# Perform hyperparameter tuning using GridSearchCV\n",
    "param_grid = {\n",
    "    \"n_estimators\": [100, 200, 300],\n",
    "    \"max_depth\": [None, 10, 20],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "}\n",
    "\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(rf_classifier, param_grid, scoring=\"accuracy\", cv=3, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(train_embeddings_balanced, train_labels_balanced)\n",
    "\n",
    "# Best Random Forest model\n",
    "rf_classifier = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on validation set\n",
    "val_predictions = rf_classifier.predict(val_embeddings)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "print(\"Validation Accuracy:\", val_accuracy)\n",
    "print(\"Validation Classification Report:\")\n",
    "print(classification_report(val_labels, val_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4c74c3ac-d3a8-4735-84a6-c5f5f889fa80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.75\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.89      0.85       895\n",
      "           1       0.23      0.13      0.17       209\n",
      "\n",
      "    accuracy                           0.75      1104\n",
      "   macro avg       0.52      0.51      0.51      1104\n",
      "weighted avg       0.70      0.75      0.72      1104\n",
      "\n",
      "Predictions saved to stock_price_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on test set\n",
    "test_predictions = rf_classifier.predict(test_embeddings)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(test_labels, test_predictions))\n",
    "\n",
    "# Save the predictions for analysis\n",
    "output = pd.DataFrame(test_data)  # Include all columns from the test dataset\n",
    "output[\"Predicted Label\"] = test_predictions\n",
    "output.to_csv(\"stock_price_predictions.csv\", index=False)\n",
    "print(\"Predictions saved to stock_price_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ce50a1-3a85-4c80-974a-266df0558186",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10191a75-0338-4a2c-9566-d4e373309f89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb73b314-c02e-4da2-be4e-ffab3d801b90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cda9c9-b42a-493c-bee0-2f0601de0a8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cf15ac-2258-44bd-9819-1a8f1b2fc0d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9483e11b-0c6c-4846-bd86-3e7380692157",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384236ee-288c-45fb-bcec-e84563e84402",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
